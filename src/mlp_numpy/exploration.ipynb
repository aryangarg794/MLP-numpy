{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import functools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try to keep the code torchlike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class for other classes to subclass\n",
    "class Module:\n",
    "\n",
    "    def __init__(self, cls) -> None:\n",
    "        functools.update_wrapper(self, cls)\n",
    "\n",
    "    def __call__(self, *args):\n",
    "        return self.forward(*args)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f'{self.__class__.__name__}()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CELoss(Module):\n",
    "    \"\"\" Cross entropy loss for multiclass prediction\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(self)\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(y_true: np.ndarray, y_pred: np.ndarray) -> float | np.ndarray:\n",
    "        epsilon = 1e-12\n",
    "        y_pred = np.clip(y_pred, epsilon, 1. - epsilon)\n",
    "        \n",
    "        batch_size = y_true.shape[0]\n",
    "        loss = -np.sum(y_true * np.log(y_pred)) / batch_size\n",
    "        \n",
    "        return loss\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    \"\"\" ReLU activation function\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(self)\n",
    "        self.x = None\n",
    "    \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        self.x = x\n",
    "        return np.fmax(0, self.x)\n",
    "    \n",
    "    def backward(self, upstream_grad: np.ndarray) -> np.ndarray:\n",
    "        local_grad = np.where(self.x > 0, 1, 0)\n",
    "        return [], [], upstream_grad * local_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(Module):\n",
    "    \"\"\" Softmax activation function\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(self)\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(logits: np.ndarray) -> np.ndarray:\n",
    "        exps = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
    "        return exps / np.sum(exps, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    \"\"\" Single linear layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size: int, output_size: int) -> None:\n",
    "        super().__init__(self)\n",
    "        \n",
    "        self.W = np.random.randn(output_size, input_size) * 0.01 \n",
    "        self.b = np.zeros((output_size,))\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.z = None\n",
    "        self.x = None\n",
    "        \n",
    "        self.W_grad = np.zeros_like(self.W)\n",
    "        self.b_grad = np.zeros_like(self.b)\n",
    "        \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        self.x = x.copy()\n",
    "        self.z = self.x @ self.W.T + self.b\n",
    "        return self.z\n",
    "    \n",
    "    def backward(self, upstream_grad: np.ndarray) -> np.ndarray:\n",
    "        # print(f'W {self.W.shape}, z: {self.z.shape}, up: {upstream_grad.shape}, x: {self.x.shape} for layer {str(self)}')\n",
    "        self.W_grad += upstream_grad.T @ self.x\n",
    "        self.b_grad += np.sum(upstream_grad, axis=0)\n",
    "        return self.W_grad, self.b_grad, upstream_grad @ self.W\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.W_grad = np.zeros_like(self.W)\n",
    "        self.b_grad = np.zeros_like(self.b)\n",
    "    \n",
    "    def parameters(self) -> list:\n",
    "        return [self.W, self.b]\n",
    "    \n",
    "    def step(self, w_grad: np.ndarray, b_grad: np.ndarray, lr: float) -> None:\n",
    "        self.W -= lr * w_grad \n",
    "        self.b -= lr * b_grad\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return f'Linear(inp={self.input_size}, out={self.output_size})'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(Module):\n",
    "    \"\"\" Multi layer perceptron\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_layer_sizes: list | tuple, input_size: int, output_size: int) -> None:\n",
    "        super().__init__(self)\n",
    "        \n",
    "        self.layers = []\n",
    "        self.layers.append(Linear(input_size=input_size, output_size=hidden_layer_sizes[0]))\n",
    "        self.layers.append(ReLU())\n",
    "        \n",
    "        for i in range(len(hidden_layer_sizes)-1):\n",
    "            self.layers.append(Linear(input_size=hidden_layer_sizes[i], output_size=hidden_layer_sizes[i+1]))\n",
    "            self.layers.append(ReLU())\n",
    "        \n",
    "        self.layers.append(Linear(input_size=hidden_layer_sizes[-1], output_size=output_size))\n",
    "        \n",
    "        self.logits = None\n",
    "    \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)   \n",
    "        self.logits = x.copy()\n",
    "        return x\n",
    "    \n",
    "    def parameters(self) -> list:\n",
    "        params = []\n",
    "        for layer in self.layers[::-1]:\n",
    "            if hasattr(layer, 'W'):\n",
    "                params += layer.parameters()\n",
    "        return params\n",
    "    \n",
    "    def compute_loss(self, y_true: np.ndarray, y_pred: np.ndarray, from_logits=True) -> float:\n",
    "        if from_logits:\n",
    "            y_pred = Softmax.forward(y_pred)\n",
    "        return CELoss.forward(y_true, y_pred)\n",
    "        \n",
    "    def backward(self, y_true: np.ndarray) -> np.ndarray: \n",
    "        n = y_true.shape[0]\n",
    "        upstream_grad = Softmax.forward(self.logits) - y_true\n",
    "        upstream_grad /= n\n",
    "        out_grads = []\n",
    "        for layer in self.layers[::-1]:\n",
    "            w_grad, b_grad, upstream_grad = layer.backward(upstream_grad)\n",
    "            if len(w_grad) > 0:\n",
    "                out_grads += [w_grad]\n",
    "                out_grads += [b_grad]\n",
    "        return out_grads\n",
    "    \n",
    "    def zero_grad(self) -> None:\n",
    "        for layer in self.layers[::-1]:\n",
    "            if hasattr(layer, 'W'):\n",
    "                layer.zero_grad()\n",
    "    \n",
    "    def optimizer_step(self, grads: np.ndarray, lr: float) -> None:\n",
    "        idx = 0 \n",
    "        for layer in self.layers[::-1]:\n",
    "            if hasattr(layer, 'W'):\n",
    "                w_grad = grads[idx]\n",
    "                b_grad = grads[idx+1]\n",
    "                layer.step(w_grad, b_grad, lr)\n",
    "                idx += 2\n",
    "                \n",
    "    def __repr__(self) -> str:\n",
    "        return f'=======Layers=======\\n{'\\n'.join(str(layer) for layer in self.layers)}'\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======Layers=======\n",
      "Linear(inp=768, out=1)\n",
      "ReLU()\n",
      "Linear(inp=1, out=10)\n",
      "\n",
      "Predicted output (logits):\n",
      " (32, 10)\n",
      "Loss: 2.302304993926562\n",
      "Gradients after backward pass:\n",
      "(10, 1) (10, 1)\n",
      "(10,) (10,)\n",
      "(1, 768) (1, 768)\n",
      "(1,) (1,)\n"
     ]
    }
   ],
   "source": [
    "# testing environment\n",
    "np.random.seed(42)\n",
    "\n",
    "input_data = np.random.randn(32, 768)\n",
    "y_true = np.zeros((32, 10))\n",
    "labels = np.random.randint(0, 10, size=(32,))\n",
    "y_true[np.arange(32), labels] = 1\n",
    "\n",
    "\n",
    "mlp = MLP([1], 768, 10)\n",
    "print(mlp)\n",
    "\n",
    "y_pred = mlp.forward(input_data)\n",
    "\n",
    "loss = mlp.compute_loss(y_true, y_pred)\n",
    "\n",
    "grads = mlp.backward(y_true)\n",
    "params = mlp.parameters()\n",
    "\n",
    "print(\"\")\n",
    "print(\"Predicted output (logits):\\n\", y_pred.shape)\n",
    "print(\"Loss:\", loss)\n",
    "print(\"Gradients after backward pass:\")\n",
    "for grad, param in zip(grads, params):\n",
    "    print(grad.shape, param.shape)\n",
    "    \n",
    "mlp.optimizer_step(grads, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======Layers=======\n",
      "Linear(inp=10, out=5)\n",
      "ReLU()\n",
      "Linear(inp=5, out=5)\n",
      "ReLU()\n",
      "Linear(inp=5, out=3)\n",
      "loss for custom: 0.9984, loss for torch: 0.9984\n",
      "custom: [[-0.2124254  -0.02882922 -0.27971405]\n",
      " [-0.21084696 -0.01194282 -0.23850769]\n",
      " [-0.2147279  -0.00732934 -0.2478374 ]\n",
      " [-0.32275128  0.11145934 -0.20302606]]\n",
      "torch: tensor([[-0.2124, -0.0288, -0.2797],\n",
      "        [-0.2108, -0.0119, -0.2385],\n",
      "        [-0.2147, -0.0073, -0.2478],\n",
      "        [-0.3228,  0.1115, -0.2030]], grad_fn=<AddmmBackward0>)\n",
      "custom shape: (5,), torch shape: torch.Size([5, 10])\n",
      "custom shape: (5, 10), torch shape: torch.Size([5])\n",
      "custom shape: (5,), torch shape: torch.Size([5, 5])\n",
      "custom shape: (5, 5), torch shape: torch.Size([5])\n",
      "custom shape: (3,), torch shape: torch.Size([3, 5])\n",
      "custom shape: (3, 5), torch shape: torch.Size([3])\n",
      "Custom Gradient:  (5,)\n",
      "Torch Gradient:  (5,)\n",
      "Allclose: True\n",
      "Difference:  3.7252904e-10 \n",
      "\n",
      "Custom Gradient:  (5, 10)\n",
      "Torch Gradient:  (5, 10)\n",
      "Allclose: True\n",
      "Difference:  6.146729e-10 \n",
      "\n",
      "Custom Gradient:  (5,)\n",
      "Torch Gradient:  (5,)\n",
      "Allclose: True\n",
      "Difference:  2.9802323e-09 \n",
      "\n",
      "Custom Gradient:  (5, 5)\n",
      "Torch Gradient:  (5, 5)\n",
      "Allclose: True\n",
      "Difference:  1.2293458e-09 \n",
      "\n",
      "Custom Gradient:  (3,)\n",
      "Torch Gradient:  (3,)\n",
      "Allclose: True\n",
      "Difference:  2.7318796e-08 \n",
      "\n",
      "Custom Gradient:  (3, 5)\n",
      "Torch Gradient:  (3, 5)\n",
      "Allclose: True\n",
      "Difference:  1.6763806e-09 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "class TorchMLP(nn.Module):\n",
    "    def __init__(self, hidden_layer_sizes, input_size, output_size):\n",
    "        super(TorchMLP, self).__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(input_size, hidden_layer_sizes[0]))\n",
    "        layers.append(nn.ReLU())\n",
    "        \n",
    "        for i in range(len(hidden_layer_sizes) - 1):\n",
    "            layers.append(nn.Linear(hidden_layer_sizes[i], hidden_layer_sizes[i + 1]))\n",
    "            layers.append(nn.ReLU())\n",
    "        \n",
    "        layers.append(nn.Linear(hidden_layer_sizes[-1], output_size))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "def one_hot_encod(labels, num_classes):\n",
    "    return np.eye(num_classes)[labels]\n",
    "\n",
    "batch_size = 4\n",
    "input_size = 10\n",
    "output_size = 3\n",
    "hidden_layers = [5, 5]\n",
    "\n",
    "np_input = np.random.randn(batch_size, input_size).astype(np.float32)\n",
    "np_labels = np.random.randint(0, output_size, batch_size)\n",
    "one_hot_labels = one_hot_encod(np_labels, output_size)\n",
    "\n",
    "torch_input = torch.tensor(np_input)\n",
    "torch_labels = torch.tensor(np_labels)\n",
    "\n",
    "custom_mlp = MLP(hidden_layer_sizes=hidden_layers, input_size=input_size, output_size=output_size)\n",
    "torch_mlp = TorchMLP(hidden_layer_sizes=hidden_layers, input_size=input_size, output_size=output_size)\n",
    "\n",
    "print(custom_mlp)\n",
    "\n",
    "for layer_c, layer_t in zip(custom_mlp.layers, torch_mlp.net):\n",
    "    if hasattr(layer_c, 'W') and hasattr(layer_t, 'weight'):\n",
    "        layer_c.W = layer_t.weight.detach().numpy()\n",
    "        layer_c.b = layer_t.bias.detach().numpy()\n",
    "\n",
    "custom_preds = custom_mlp.forward(np_input)\n",
    "\n",
    "torch_preds = torch_mlp(torch_input)\n",
    "\n",
    "custom_loss = custom_mlp.compute_loss(one_hot_labels, custom_preds)\n",
    "torch_loss_fn = nn.CrossEntropyLoss()\n",
    "torch_loss = torch_loss_fn(torch_preds, torch_labels)\n",
    "\n",
    "custom_mlp.zero_grad()\n",
    "custom_grads = custom_mlp.backward(one_hot_labels)\n",
    "\n",
    "torch_loss.backward()\n",
    "\n",
    "torch_grads = [param.grad for param in torch_mlp.parameters()]\n",
    "\n",
    "print(f'loss for custom: {custom_loss:.4f}, loss for torch: {torch_loss:.4f}')\n",
    "print(f'custom: {custom_preds}\\ntorch: {torch_preds}')\n",
    "\n",
    "for c_grad, t_grad in zip(custom_grads[::-1], torch_grads):\n",
    "    print(f'custom shape: {c_grad.shape}, torch shape: {t_grad.shape}')\n",
    "\n",
    "def compare_gradients(c_grad, t_grad):\n",
    "    print(\"Custom Gradient: \", c_grad.shape)\n",
    "    print(\"Torch Gradient: \", t_grad.detach().numpy().shape)\n",
    "    print(f\"Allclose: {np.allclose(c_grad, t_grad)}\")\n",
    "    print(\"Difference: \", np.abs(c_grad - t_grad.detach().numpy()).mean(), '\\n')\n",
    "\n",
    "# compare gradients\n",
    "compare_gradients(custom_grads[::-1][0], torch_grads[1])\n",
    "compare_gradients(custom_grads[::-1][1], torch_grads[0])\n",
    "compare_gradients(custom_grads[::-1][2], torch_grads[3])\n",
    "compare_gradients(custom_grads[::-1][3], torch_grads[2])\n",
    "compare_gradients(custom_grads[::-1][4], torch_grads[5])\n",
    "compare_gradients(custom_grads[::-1][5], torch_grads[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (60000, 784)\n",
      "Test data shape: (10000, 784)\n",
      "Training labels shape: (60000, 10)\n",
      "Test labels shape: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# get data\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def one_hot_encode(y, num_classes):\n",
    "    one_hot = np.zeros((y.size, num_classes))\n",
    "    one_hot[np.arange(y.size), y] = 1\n",
    "    return one_hot\n",
    "\n",
    "def mnist_to_numpy(is_train=True):\n",
    "    dataset = datasets.MNIST(root='./data', train=is_train, download=True,\n",
    "                             transform=transforms.ToTensor())\n",
    "    \n",
    "    data = []\n",
    "    labels = []\n",
    "    for image, label in dataset:\n",
    "        flattened = image.numpy().flatten()\n",
    "        data.append(flattened)\n",
    "        labels.append(label)\n",
    "    \n",
    "    labels = np.array(labels)\n",
    "    labels = one_hot_encode(labels, 10)\n",
    "    return np.array(data), labels\n",
    "\n",
    "X_train, y_train = mnist_to_numpy(is_train=True)\n",
    "X_test, y_test = mnist_to_numpy(is_train=False)\n",
    "\n",
    "print(\"Training data shape:\", X_train.shape)\n",
    "print(\"Test data shape:\", X_test.shape)\n",
    "print(\"Training labels shape:\", y_train.shape)\n",
    "print(\"Test labels shape:\", y_test.shape)\n",
    "\n",
    "def get_mini_batches(X, y, batch_size, num_iter):\n",
    "    num_samples = X.shape[0]\n",
    "    indices = np.arange(num_samples)\n",
    "    \n",
    "    for i in range(num_iter):\n",
    "        if i % (num_samples // batch_size) == 0:\n",
    "            np.random.shuffle(indices)\n",
    "        \n",
    "        start_idx = (i * batch_size) % num_samples\n",
    "        batch_indices = indices[start_idx:start_idx + batch_size]\n",
    "        \n",
    "        yield X[batch_indices], y[batch_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP([1000], 784, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100 | loss: 0.0546\n",
      "Iteration 200 | loss: 0.1158\n"
     ]
    }
   ],
   "source": [
    "# training procedure \n",
    "batch_size = 256\n",
    "num_iterations = 200\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "for i, (batch_X, batch_y) in enumerate(get_mini_batches(X_train, y_train, batch_size, num_iterations)):\n",
    "    logits = model.forward(batch_X)\n",
    "    model.zero_grad()\n",
    "    \n",
    "    loss = model.compute_loss(batch_y, logits)\n",
    "    grads = model.backward(batch_y)\n",
    "    \n",
    "    lr = 0.1 if i < num_iterations // 2 else 0.01\n",
    "    model.optimizer_step(grads, lr=lr)\n",
    "    \n",
    "    if i % 100 == 99: \n",
    "        print(f'Iteration {i+1} | loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.16%\n"
     ]
    }
   ],
   "source": [
    "# get accuracy on test set\n",
    "np.random.seed(42)\n",
    "\n",
    "logits_test = model(X_test)\n",
    "# print(f'logits shape: {logits_test.shape}')\n",
    "y_preds = Softmax.forward(logits_test)\n",
    "# print(f'logits shape: {y_preds.shape}')\n",
    "\n",
    "pred_labels = np.argmax(y_preds, axis=1)\n",
    "true_labels = np.argmax(y_test, axis=1)\n",
    "# print(f'{pred_labels.shape}, {true_labels.shape}')\n",
    "\n",
    "\n",
    "correct = np.sum(pred_labels == true_labels)\n",
    "acc = (correct / len(pred_labels)) * 100\n",
    "print(f'Accuracy: {acc}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
